{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing: Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <br>\n",
    "The aim of this notebook is to be able to classify tweets into two categories, tweets that are about actual disasters and tweets that are not about disasters. This will be done using NLP and machine learning, on a dataset of 10,000 tweets. \n",
    "\n",
    "**DISCLAIMER: SOME TWEETS MAY CONTAIN EXPLICIT LANGUAGE**\n",
    "\n",
    "**What is NLP?**<br>\n",
    "\n",
    "NLP stands for Natural Language Processing, this is defined by the Oxford Languages Dictionary as 'The application of computational techniques to the analysis and synthesis of natural language and speech'. Natural language is the way humans talk to each other through things such as text and speech.<br>\n",
    "\n",
    "Language can be incredibly difficult, it is a thing that takes each one of us years to learn and even then we often make mistakes throughout our lives. NLP as a tool is very powerful but it faces many limitations and problems; such as the ability to understand context, sarcasm, errors in the text, slang, and ambiguity. This can have an impact on how our models may decide the outcome of the tweet so we can already predict that it will make some mistakes when trying to predict the outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started! \n",
    "\n",
    "The first steps like in every project are to import our libraries and then read in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import io \n",
    "import re\n",
    "import string \n",
    "import unicodedata\n",
    "import spacy \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from spellchecker import SpellChecker\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to read in our dataset and store it in a dataframe so we can easily access and manipulate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "tweets = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
       "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
       "7  22     NaN      NaN                                  Hey! How are you?\n",
       "8  27     NaN      NaN                                   What a nice hat?\n",
       "9  29     NaN      NaN                                          Fuck off!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at what the data looks like \n",
    "tweets.head(10)\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 7613, Columns: 5 in the training data.\n",
      "Rows: 3263, Columns: 4 in the test data.\n"
     ]
    }
   ],
   "source": [
    "print(f'Rows: {tweets.shape[0]}, Columns: {tweets.shape[1]} in the training data.')\n",
    "print(f'Rows: {test.shape[0]}, Columns: {test.shape[1]} in the test data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the test data is missing the target column, but our training data includes this. So the end result is to predict the target column for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets see what the target looks like \n",
    "tweets.describe()\n",
    "tweets.target.value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more tweets with no mention of disaster than tweets with a disaster. This is known as a class imbalance, machine learning models work best when the outcome classes are about equal. It isn't a huge difference so maybe we can play about with reducing this imbalance later if we don't get the results we are hoping for. We will consider a range of measures when trying to determine the performance of the models as accuracy can be flawed when there is a class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at an example tweet\n",
    "tweets['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning \n",
    "\n",
    "The next step should be to clean the data and make it as usable as possible. I will first make a copy of the data just to be on the safe side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine both test and train data into a new df for cleaning \n",
    "df = pd.concat([tweets, test])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtags are a common feature of many tweets and are often used inline, so the symbols should be stripped along with any other special characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test string to check the data cleaning process\n",
    "test = \"[This is just to test] somerandom@text.totest, https://maybe.the/tweet/contains.alink<br>😔, I am a #bad #person corevt the word?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any links \n",
    "def remove_hyperlink(data):\n",
    "    isurl = re.compile(r'https?://\\S+|www.\\.\\S+')\n",
    "    return isurl.sub(r'', data)\n",
    "\n",
    "# Remove any special characters\n",
    "def remove_special_chars(data):\n",
    "    data = re.sub('[,\\.!:;()\"]', ' ', data)\n",
    "    data = re.sub('[^a-zA-Z\"]', ' ', data)\n",
    "    data = re.sub('\\[[^]]*\\]', ' ', data)\n",
    "    return data\n",
    "\n",
    "# Make Lower Case\n",
    "def make_lower(data):\n",
    "    data = data.str.lower()\n",
    "    return data\n",
    "\n",
    "# Remove any HTML\n",
    "def remove_html(data):\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# Remove any Emojis\n",
    "def remove_emojis(data):\n",
    "    emojis = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return emojis.sub(r'',data)\n",
    "\n",
    "# Remove double spaces\n",
    "def remove_doublespaces(data):\n",
    "    data = re.sub('  ', ' ', data)\n",
    "    data = re.sub('  ', ' ', data)\n",
    "    return data\n",
    "\n",
    "# Correct Mispellings \n",
    "spell = SpellChecker()\n",
    "def correct_spelling(data):\n",
    "    corrected = []\n",
    "    mispellings = spell.unknown(data.split())\n",
    "    for word in data.split():\n",
    "        if word in mispellings:\n",
    "            corrected.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected.append(word)\n",
    "    return \" \".join(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Call these functions \\ntest = remove_hyperlink(test)\\ntest = remove_special_chars(test)\\ntest = make_lower(test)\\ntest = remove_html(test)\\ntest = remove_emojis(test)\\ntest = remove_doublespaces(test)\\ntest = correct_spelling(test)\\ntest'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Call these functions \n",
    "test = remove_hyperlink(test)\n",
    "test = remove_special_chars(test)\n",
    "test = make_lower(test)\n",
    "test = remove_html(test)\n",
    "test = remove_emojis(test)\n",
    "test = remove_doublespaces(test)\n",
    "test = correct_spelling(test)\n",
    "test'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we know that the data cleanining functions work as expected we can apply them to our combibned data\n",
    "df['text'] = df['text'].apply(remove_hyperlink)\n",
    "df['text'] = df['text'].apply(remove_special_chars)\n",
    "df['text'] = make_lower(df['text'])\n",
    "df['text'] = df['text'].apply(remove_html)\n",
    "df['text'] = df['text'].apply(remove_emojis)\n",
    "df['text'] = df['text'].apply(remove_doublespaces)\n",
    "df['text'] = df['text'].apply(correct_spelling)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
